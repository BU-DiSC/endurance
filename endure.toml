# =============================================================================
# HEADER APP
#   Logic of app including jobs list to run
# =============================================================================
[app]
name = "ENDURE"
jobs = [
    # 'DataGen',
    'Train',
]

# =============================================================================
# HEADER LOGGER
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[log]
name = 'endure-logger'
format = "[%(levelname)s][%(asctime)-15s][%(filename)s] %(message)s"
datefmt = '%d-%m-%y:%H:%M:%S'
level = "INFO"
# =============================================================================
# HEADER IO
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[io]
data_dir = "/data"

# =============================================================================
# HEADER LSM
#   Bounds for searching the LSM space
# =============================================================================
[lsm]
max_levels = 16

[lsm.size_ratio]
max = 50
min = 2

[lsm.bits_per_elem]
max = 9.5
min = 0

# =============================================================================
# HEADER DATAGEN
#   Generating data to be used for learning
# =============================================================================
[data]
# h, z0, z1, q, w
mean_bias = [4.75, 0.5, 0.5, 0.5, 0.5]
std_bias =  [2.74,  0.3, 0.3, 0.3, 0.3]

[data.gen]
format = 'parquet'
generator = 'QCost'
dir = 'test-data/qcost-parquet'
file_prefix = 'qcost'
# Parallel workers, -1 will force all cores to be used
num_workers = 32
num_files = 32
precision = 3
# Per file sample
samples = 262144

# =============================================================================
# HEADER MODEL
#   Configuration parameters for creating, training, and validating a model
# =============================================================================
[model]
# Architecture to pick
arch = "QCost"

# Name to save the model
dir = 'qcost-11-08'

# Output dimensions, should match whatever the cost function outputs
out_dims = 4

# Number of continuous variables the NN will take, referring to bits per
# element, workload, etc
num_cont_vars = 5

# Number of categorical variables we will need to encode, referring to size
# ratios and number of files per level (if using that cost-model)
num_cate_vars = 2

# Hidden layers between the continous inputs + embedding to output
hidden_layers = 8

# Size of our cateogrical variable embedding, our overall cost-model network
# input will be
#   num_cont_vars + (num_cate_vars * embedding_size)
# As each categorical variable will be embedded down to size `embedding_size`
embedding_size = 8

[train]
batch_size = 4096
drop_last = true
shuffle = true
# Number of iterations of non-improvement to trigger and early stop of training
early_stop_num = 10
# Delta between losses to count as a non-improvement
epsilon = 1e-5
# Maximum number of epochs we will train over
max_epochs = 128

# If training is via SGD, this will be the gamma decay parameter for learning
learning_rate_decay = 0.99
learning_rate = 0.0001

[train.data]
format = 'parquet'
# directory for data used for training data
dir = "train-data/qcost-parquet"
# use datapipe implementation (streaming) or dataset (load all in RAM)
use_dp = false
num_workers = 4

[test]
batch_size = 8192
drop_last = true
shuffle = false

[test.data]
dir = "test-data/qcost-parquet"
format = 'parquet'
use_dp = false
num_workers = 2

# =============================================================================
# HEADER SYSTEM
#   System settings encapsulate physical components of the machine you're
#   expecting to run your LSM tree on, and additionally starting budgets and
#   number of element.
# =============================================================================
[system]
# Number of physical entries per page
B = 4

# Read/Write asymmetry coefficient, 1 implies writes and reads throughput is
# roughly the same the current storage device
phi = 1

# Range query selectivity, 0 implies the key range per range query would roughly
# fit into 1 page. Selectivity 1 implies range queries will always query the
# full key possible range
s = 0.00

# Size of a single entry in bytes
E = 1024

# Memory budget in terms of bits per element, this combined with number of
# elements (N) will get you roughly the expected total memory we are allocating
# between bloom filter memory and buffer memory
H = 10

# Total number of elements our LSM tree will start off with
N = 100000000

# =============================================================================
# HEADER WORKLOADS
#     List of representative workloads
# =============================================================================
[[workloads]]
id = 0
z0 = 0.25
z1 = 0.25
q  = 0.25
w  = 0.25

[[workloads]]
id = 1
z0 = 0.97
z1 = 0.01
q = 0.01
w = 0.01

[[workloads]]
id = 2
z0 = 0.01
z1 = 0.97
q = 0.01
w = 0.01

[[workloads]]
id = 3
z0 = 0.01
z1 = 0.01
q = 0.97
w = 0.01

[[workloads]]
id = 4
z0 = 0.01
z1 = 0.01
q = 0.01
w = 0.97

[[workloads]]
id = 5
z0 = 0.49
z1 = 0.49
q = 0.01
w = 0.01

[[workloads]]
id = 6
z0 = 0.49
z1 = 0.01
q = 0.49
w = 0.01

[[workloads]]
id = 7
z0 = 0.49
z1 = 0.01
q = 0.01
w = 0.49

[[workloads]]
id = 8
z0 = 0.01
z1 = 0.49
q = 0.49
w = 0.01

[[workloads]]
id = 9
z0 = 0.01
z1 = 0.49
q = 0.01
w = 0.49

[[workloads]]
id = 10
z0 = 0.01
z1 = 0.01
q = 0.49
w = 0.49

[[workloads]]
id = 11
z0 = 0.33
z1 = 0.33
q = 0.33
w = 0.01

[[workloads]]
id = 12
z0 = 0.33
z1 = 0.33
q = 0.01
w = 0.33

[[workloads]]
id = 13
z0 = 0.33
z1 = 0.01
q = 0.33
w = 0.33

[[workloads]]
id = 14
z0 = 0.01
z1 = 0.33
q = 0.33
w = 0.33
