# =============================================================================
# HEADER APP
#   Logic of app including jobs list to run
# =============================================================================
[app]
name = "ENDURE"
jobs = [
    'DataGen',
    # 'Train',
]

# =============================================================================
# HEADER LOGGER
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[log]
name = 'endure-logger'
format = "[%(levelname)s][%(asctime)-15s][%(filename)s] %(message)s"
datefmt = '%d-%m-%y:%H:%M:%S'
level = "INFO"

# =============================================================================
# HEADER IO
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[io]
data_dir = "/Users/ndhuynh/sandbox/data"

# =============================================================================
# HEADER LSM
#   Bounds for searching the LSM space
# =============================================================================
[lsm]
max_levels = 16

[lsm.size_ratio]
max = 50
min = 2

[lsm.bits_per_elem]
max = 9.5
min = 0

# =============================================================================
# HEADER DATAGEN
#   Generating data to be used for learning
# =============================================================================
[data]
# h, z0, z1, q, w
mean_bias = [4.75, 0.5, 0.5, 0.5, 0.5]
std_bias =  [2.74,  0.3, 0.3, 0.3, 0.3]

[data.gen]
format = 'parquet'
generator = 'QCost'
dir = 'train-data/qcost-parquet'
file_prefix = 'qcost'
num_workers = -1  # -1 forces all cores to be used
num_files = 16
precision = 3
samples = 262144  # per file sample

# =============================================================================
# HEADER MODEL
#   Configuration parameters for creating, training, and validating a model
# =============================================================================
[model]
# Architecture to pick
#   QCost - all levels same hold the same max files per level
#   KCost - multiple different max files per level
#   TierLevelCost - specifically for tiering and leveling
arch = "QCost"

# Name to save the model
dir = 'qcost-11-19'
out_dims = 4  # should match cost function output (z0, z1, q, h)

# Number of continuous variables the NN will take, referring to bits per
# element, workload, etc
num_cont_vars = 5

# Number of categorical variables we will need to encode, referring to size
# ratios and number of files per level (if using that cost-model)
num_cate_vars = 2

# Hidden layers between the continous inputs + embedding to output
hidden_layers = 16

# Size of our cateogrical variable embedding, our overall cost-model network
# input will be
#   num_cont_vars + (num_cate_vars * embedding_size)
# As each categorical variable will be embedded down to size `embedding_size`
embedding_size = 16


[train]
batch_size = 4096
shuffle = true
drop_last = true  # drop last if unable to make full batch
max_epochs = 50
use_gpu_if_avail = true  # if gpu is avail, use for training

# Different loss functions to train via
#   MSE - Mean squared error
#   NMSE - Normalized mean squared error
#   MSLE - Mean squared log error
#   RMSE - Root mean square error
#   RMSLE - Root mean squared log error
loss_fn = 'MSLE'

# List of optimizers supported
#   SGD - Stochastic gradient descent
#   Adam
optimizer = 'SGD'

# Learning rate schedulers
#   CosineAnnealing
#   None - Will keep learning rate constant without any decay
lr_scheduler = 'CosineAnnealing'
learning_rate = 0.0001

# If decided to use cosine annealing, all settings will be taken from this dict
# refer to pytorch docs for keywords as this dictionary will be passed by kwargs
# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html
[train.scheduler.CosineAnnealingLR]
T_max = 10
eta_min = 0  # minimum learning rate

# Early stopping will count the number of occurances where non improvement
# happens and stop once we hit a certain threshold
[train.early_stop]
enabled = false
threshold = 10  # number of non improvements to stop
epsilon = 1e-6  # delta that counts as non improvement

[train.data]
num_workers = 4
dir = "train-data/qcost-parquet"
use_dp = false  # use datapipe (streamed) or dataset (legacy)
# Format of the file to use
#   csv
#   parquet
format = 'parquet'

[test]
batch_size = 8192
drop_last = true
shuffle = false

[test.data]
num_workers = 2
format = 'parquet'
dir = "test-data/qcost-parquet"
use_dp = false

# =============================================================================
# HEADER SYSTEM
#   System settings encapsulate physical components of the machine you're
#   expecting to run your LSM tree on, and additionally starting budgets and
#   number of element.
# =============================================================================
[system]
# Number of physical entries per page
B = 4

# Read/Write asymmetry coefficient, 1 implies writes and reads throughput is
# roughly the same the current storage device
phi = 1

# Range query selectivity, 0 implies the key range per range query would roughly
# fit into 1 page. Selectivity 1 implies range queries will always query the
# full key possible range
s = 0.00

# Size of a single entry in bytes
E = 1024

# Memory budget in terms of bits per element, this combined with number of
# elements (N) will get you roughly the expected total memory we are allocating
# between bloom filter memory and buffer memory
H = 10

# Total number of elements our LSM tree will start off with
N = 100000000

# =============================================================================
# HEADER WORKLOADS
#     List of representative workloads
# =============================================================================
[[workloads]]
id = 0
z0 = 0.25
z1 = 0.25
q  = 0.25
w  = 0.25

[[workloads]]
id = 1
z0 = 0.97
z1 = 0.01
q = 0.01
w = 0.01

[[workloads]]
id = 2
z0 = 0.01
z1 = 0.97
q = 0.01
w = 0.01

[[workloads]]
id = 3
z0 = 0.01
z1 = 0.01
q = 0.97
w = 0.01

[[workloads]]
id = 4
z0 = 0.01
z1 = 0.01
q = 0.01
w = 0.97

[[workloads]]
id = 5
z0 = 0.49
z1 = 0.49
q = 0.01
w = 0.01

[[workloads]]
id = 6
z0 = 0.49
z1 = 0.01
q = 0.49
w = 0.01

[[workloads]]
id = 7
z0 = 0.49
z1 = 0.01
q = 0.01
w = 0.49

[[workloads]]
id = 8
z0 = 0.01
z1 = 0.49
q = 0.49
w = 0.01

[[workloads]]
id = 9
z0 = 0.01
z1 = 0.49
q = 0.01
w = 0.49

[[workloads]]
id = 10
z0 = 0.01
z1 = 0.01
q = 0.49
w = 0.49

[[workloads]]
id = 11
z0 = 0.33
z1 = 0.33
q = 0.33
w = 0.01

[[workloads]]
id = 12
z0 = 0.33
z1 = 0.33
q = 0.01
w = 0.33

[[workloads]]
id = 13
z0 = 0.33
z1 = 0.01
q = 0.33
w = 0.33

[[workloads]]
id = 14
z0 = 0.01
z1 = 0.33
q = 0.33
w = 0.33
