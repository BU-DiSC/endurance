# =============================================================================
# ENDURE Configuration File
#   Every job will contain it's own HEADER <Job Title> with settings appropiate
#   for the job.
#
# HEADER APP
#   Logic of app including jobs list to run
# =============================================================================
[app]
name = "ENDURE"

# =============================================================================
# HEADER LOGGER
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[log]
name = 'endure-logger'
format = "[%(levelname)s][%(asctime)-15s][%(filename)s] %(message)s"
datefmt = '%d-%m-%y:%H:%M:%S'
level = "DEBUG"
disable_tqdm = false

# =============================================================================
# HEADER IO
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[io]
data_dir = "/scratchNVMe/ndhuynh/data"

# =============================================================================
# HEADER JOB
#   Settings for each individual job (executable)
# =============================================================================
[job]
to_run = [
    # "LCMDataGen",
    "LCMTrain",
    # "LTuneDataGen",
    # "LTuneTrain",
    # TODO: "CreateTunings",
    # TODO: "RunExperiments",
]

# -----------------------------------------------------------------------------
[job.LCMDataGen]
# -----------------------------------------------------------------------------
format = "parquet"
generator = "QCost"
dir = "test-data/qcost-t30"
file_prefix = "cost"
num_workers = -1 # -1 forces all cores to be used
num_files = 8
samples = 1048576  # per file sample
overwrite_if_exists = true

# -----------------------------------------------------------------------------
[job.LCMTrain]
# -----------------------------------------------------------------------------
max_epochs = 100
save_dir = "models/lcm/qcost-10-06-v3"
use_gpu_if_avail = true

# Different loss functions to train via
#   MSE - Mean squared error
#   NMSE - Normalized mean squared error
#   MSLE - Mean squared log error
#   RMSE - Root mean square error
#   RMSLE - Root mean squared log error
#   Huber - Huber loss
loss_fn = "Huber"

# Supported optimizers
#   SGD - Stochastic gradient descent
#   Adam
#   Adagrad
optimizer = "Adam"

# Learning rate schedulers
#   [CosineAnnealing, Exponential, Constant, None]
lr_scheduler = "CosineAnnealing"

# Stop checkpointing to improve training
no_checkpoint = false

[job.LCMTrain.train]
dir = "train-data/qcost-t30"
format = "parquet"
batch_size = 8192
shuffle = true
num_workers = 12
drop_last = true

[job.LCMTrain.test]
dir = "test-data/qcost-t30"
format = "parquet"
batch_size = 65536
shuffle = false
num_workers = 4
drop_last = true

# -----------------------------------------------------------------------------
[job.LTuneDataGen]
# -----------------------------------------------------------------------------
format = "parquet"
dir = "train-data/ltune/std"
file_prefix = "wl"
num_workers = -1  # -1 forces all cores to be used
num_files = 128
samples = 1048576  # per file sample
overwrite_if_exists = true

# -----------------------------------------------------------------------------
[job.LTuneTrain]
# -----------------------------------------------------------------------------
max_epochs = 50
save_dir = "models/ltune/qlsm-10-05-v1"
use_gpu_if_avail = true

# Learned cost model is our loss, input full path to checkpoint or model file
loss_fn_path = "models/lcm/qcost-10-04-v1"

# Check train.optimizer for available options
optimizer = "SGD"

# Learning rate schedulers
#   [CosineAnnealing, Exponential, Constant, None]
lr_scheduler = "Constant"

no_checkpoint = false

[job.LTuneTrain.train]
dir = "train-data/ltune/std"
format = "parquet"
batch_size = 1024
shuffle = true
num_workers = 10
drop_last = true

[job.LTuneTrain.test]
dir = "test-data/ltune/std"
format = "parquet"
batch_size = 65536
shuffle = false
num_workers = 4
drop_last = true

# =============================================================================
# HEADER LSM
#   Generic LSM settings including maximum bounds, system settings, starting
#   budget for memory, number of elements, etc
# =============================================================================
[lsm]
# Design will effect everything else down stream (e.g. choice of neural network
# architecture for learned cost model)
#   Level
#   Tier
#   KLSM
#   QLSM
#   YZLSM
# design = 'QLSMIntegerVars'
design = 'QLSM'
max_levels = 20

[lsm.size_ratio]
max = 31
min = 2

[lsm.bits_per_elem]
max = 9.9
min = 0

[lsm.system]
# Number of physical entries per page
B = 4

# Read/Write asymmetry coefficient, 1 implies writes and reads throughput is
# roughly the same the current storage device
phi = 1

# Range query selectivity, 0 implies the key range per range Exponentialquery would roughly
# fit into 1 page. Selectivity 1 implies range queries will always query the
# full key possible range
s = 2e-7

# Size of a single entry in bits
E = 8192

# Memory budget in terms of bits per element, this combined with number of
# elements (N) will get you roughly the expected total memory we are allocating
# between bloom filter memory and buffer memory
H = 10
# Can use ttotal memory instead of bits per element to budget, set M to -1 to
# use H as it will take priority
# M = 8589934592
M = -1

# Total number of elements our LSM tree will start off with
N = 1000000000
# N = 8589934592

# =============================================================================
# HEADER LCM
#   Add configurations related to learned cost models
# =============================================================================
[lcm]
desc = 'Learned Cost Model'
# Input features for different dates is as follows:
#   Classic: ["z0", "z1", "q", "w", "B", "s", "E", "H", "N", "h", "policy", "T"]
#   QLSM: ["z0", "z1", "q", "w", "B", "s", "E", "H", "N", "h", "T", "Q"]
#   KLSM: ["z0", "z1", "q", "w", "B", "s", "E", "H", "N", "h", "T", "K"]
input_features = ["z0", "z1", "q", "w", "B", "s", "E", "H", "N", "h", "T", "Q"]
output_features = ["z0_cost", "z1_cost", "q_cost", "w_cost"]

# -----------------------------------------------------------------------------
# HEADER LCM.MODELS
#   Configuration for specific models
# -----------------------------------------------------------------------------
[lcm.model]
embedding_size = 8
hidden_length = 2
hidden_width = 32

# Dropout percentage
dropout = 0.0

# Batch or Layer norm
norm_layer = "Batch"

# Used only for classic models, generally smaller than embedding size
policy_embedding_size = 4

# -----------------------------------------------------------------------------
# HEADER LCM.DATA
#   misc settings on data such as bias, precision, etc
# -----------------------------------------------------------------------------
[lcm.data]
precision = 3
normalize_inputs = false

# =============================================================================
# END LCM
# =============================================================================

# =============================================================================
# HEADER LTUNE
#   Learned tuner module
# =============================================================================
[ltune]
desc = 'Learned Tuner'
penalty_factor = 1000
input_features = ["z0", "z1", "q", "w", "B", "s", "E", "H", "N"]
output_features = ["h", "T", "Q"]

# -----------------------------------------------------------------------------
# HEADER LTUNE.MODEL
#   Model configurations
# -----------------------------------------------------------------------------
[ltune.model]
embedding_size = 8
hidden_length = 2
hidden_width = 32

# Dropout percentage
dropout = 0.0

# Batch or Layer norm
norm_layer = "Batch"

# Used only for classic models, generally smaller than embedding size
policy_embedding_size = 4

# kwargs specific to LTune models during forward pass
[ltune.model.train_kwargs]
temp = 1
hard = false

[ltune.model.test_kwargs]
temp = 0.01
hard = true

[ltune.model.classic]
normalize = 'Batch'
layer_size = 128
num_layers = 4

[ltune.model.klsm]
hidden_layers = 16

# -----------------------------------------------------------------------------
# HEADER LTUNE.DATA
#   Misc data items
# -----------------------------------------------------------------------------
[ltune.data]
precision = 3
normalize_inputs = false

# =============================================================================
# END LTUNE
# =============================================================================

# =============================================================================
# HEADER TRAIN
#   Generic settings for training options like optiimzers and schedulers
# =============================================================================
[train.scheduler.CosineAnnealingLR]
T_max = 10
eta_min = 0.0001  # minimum learning rate

[train.scheduler.Exponential]
gamma = 0.9

[train.optimizer.Adam]
lr = 0.01

[train.optimizer.SGD]
lr = 0.001

[train.optimizer.Adagrad]
lr = 0.001

# =============================================================================
# HEADER CREATETUNING
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[tunings]
# Architecture to pick
#   QCost - all levels same hold the same max files per level
#   KCost - multiple different max files per level
#   YZCost - Y files per level except the last level will be Z files
#   TierCost - Traditional model fixed to tiering policy
#   LevelCost - Traditional model fixed to leveling policy
#   TierLevelCost - Model will solve both and pick the lowest of the two
cost_model = "LevelCost"

[tunings.rho]
# Range of rhos we want to search over over
start = 0
stop = 4
step = 0.25

# =============================================================================
# HEADER GENERATOR
#     Generator settings
# =============================================================================
[generator]
page_sizes = [4, 8, 16]                     # KB pages
# selectivity_range = [1e-9, 1e-10]           # low, high
selectivity_range = [1e-7, 1e-9]            # low, high
entry_sizes = [1024, 2048, 4096, 8192]      # bits
memory_budget = [5, 20]                     # low, high, bits per element
elements_range = [100000000, 1000000000]    # element range

# =============================================================================
# HEADER WORKLOADS
#     List of representative workloads
# =============================================================================
[[workloads]]
id = 0
z0 = 0.25
z1 = 0.25
q  = 0.25
w  = 0.25

[[workloads]]
id = 1
z0 = 0.97
z1 = 0.01
q = 0.01
w = 0.01

[[workloads]]
id = 2
z0 = 0.01
z1 = 0.97
q = 0.01
w = 0.01

[[workloads]]
id = 3
z0 = 0.01
z1 = 0.01
q = 0.97
w = 0.01

[[workloads]]
id = 4
z0 = 0.01
z1 = 0.01
q = 0.01
w = 0.97

[[workloads]]
id = 5
z0 = 0.49
z1 = 0.49
q = 0.01
w = 0.01

[[workloads]]
id = 6
z0 = 0.49
z1 = 0.01
q = 0.49
w = 0.01

[[workloads]]
id = 7
z0 = 0.49
z1 = 0.01
q = 0.01
w = 0.49

[[workloads]]
id = 8
z0 = 0.01
z1 = 0.49
q = 0.49
w = 0.01

[[workloads]]
id = 9
z0 = 0.01
z1 = 0.49
q = 0.01
w = 0.49

[[workloads]]
id = 10
z0 = 0.01
z1 = 0.01
q = 0.49
w = 0.49

[[workloads]]
id = 11
z0 = 0.33
z1 = 0.33
q = 0.33
w = 0.01

[[workloads]]
id = 12
z0 = 0.33
z1 = 0.33
q = 0.01
w = 0.33

[[workloads]]
id = 13
z0 = 0.33
z1 = 0.01
q = 0.33
w = 0.33

[[workloads]]
id = 14
z0 = 0.01
z1 = 0.33
q = 0.33
w = 0.33
