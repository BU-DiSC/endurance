# =============================================================================
# ENDURE Configuration File
#   Every job will contain it's own HEADER <Job Title> with settings appropiate
#   for the job.
#
# HEADER APP
#   Logic of app including jobs list to run
# =============================================================================
[app]
name = "ENDURE"

# =============================================================================
# HEADER LOGGER
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[log]
name = 'endure-logger'
format = "[%(levelname)s][%(asctime)-15s][%(filename)s] %(message)s"
datefmt = '%d-%m-%y:%H:%M:%S'
level = "INFO"

# =============================================================================
# HEADER IO
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[io]
data_dir = "/data"

# =============================================================================
# HEADER JOB
#   Settings for each individual job (executable)
# =============================================================================
[job]
to_run = [
    # "LCMDataGen",
    # "LCMTrain",
    # "LTuneDataGen",
    "LTuneTrain",
    # TODO: "CreateTunings",
    # TODO: "RunExperiments",
]

# -----------------------------------------------------------------------------
[job.LCMDataGen]
# -----------------------------------------------------------------------------
format = "parquet"
generator = "LevelCost"
dir = "train-data/levelcost-parquet"
file_prefix = "levelcost"
num_workers = -1  # -1 forces all cores to be used
num_files = 8
samples = 262144  # per file sample
overwrite_if_exists = true

# -----------------------------------------------------------------------------
[job.LCMTrain]
# -----------------------------------------------------------------------------
max_epochs = 50
save_dir = "models/level-02-25"
use_gpu_if_avail = true

# Different loss functions to train via
#   MSE - Mean squared error
#   NMSE - Normalized mean squared error
#   MSLE - Mean squared log error
#   RMSE - Root mean square error
#   RMSLE - Root mean squared log error
loss_fn = "MSLE"

# Supported optimizers
#   SGD - Stochastic gradient descent
#   Adam
#   Adagrad
optimizer = "Adam"

# Learning rate schedulers
#   [CosineAnnealing, Constant, None]
lr_scheduler = "None"

[job.LCMTrain.train]
dir = "train-data/levelcost-parquet"
format = "parquet"
batch_size = 8192
shuffle = true
num_workers = 6
drop_last = true
use_dp = false

[job.LCMTrain.test]
dir = "test-data/levelcost-parquet"
format = "parquet"
batch_size = 262114
shuffle = false
num_workers = 2
drop_last = true
use_dp = false

# -----------------------------------------------------------------------------
[job.LTuneDataGen]
# -----------------------------------------------------------------------------
format = "parquet"
generator = "default"
dir = "test-data/workload-parquet"
file_prefix = "workload"
num_workers = -1  # -1 forces all cores to be used
num_files = 16
samples = 262144  # per file sample
overwrite_if_exists = true

# -----------------------------------------------------------------------------
[job.LTuneTrain]
# -----------------------------------------------------------------------------
max_epochs = 50
save_dir = "models/level-tuner-03-03"
use_gpu_if_avail = true

# Learned cost model is our loss, input full path to checkpoint or model file
loss_fn_path = "models/level-02-25/best.model"

# Check train.optimizer for available options
optimizer = "SGD"

# Learning rate schedulers
#   [CosineAnnealing, Constant, None]
lr_scheduler = "CosineAnnealing"

[job.LTuneTrain.train]
dir = "train-data/workload-parquet"
format = "parquet"
batch_size = 1024
shuffle = true
num_workers = 4
drop_last = true
use_dp = false

[job.LTuneTrain.test]
dir = "test-data/workload-parquet"
format = "parquet"
batch_size = 262114
shuffle = false
num_workers = 1
drop_last = true
use_dp = false

# =============================================================================
# HEADER LSM
#   Generic LSM settings including maximum bounds, system settings, starting
#   budget for memory, number of elements, etc
# =============================================================================
[lsm]
# Design will effect everything else down stream (e.g. choice of neural network
# architecture for learned cost model)
#   Level
#   Tier
#   KLSM
#   QLSM
design = 'Level'
max_levels = 20

[lsm.size_ratio]
max = 50
min = 2

[lsm.bits_per_elem]
max = 9.9
min = 0

[lsm.system]
# Number of physical entries per page
B = 4

# Read/Write asymmetry coefficient, 1 implies writes and reads throughput is
# roughly the same the current storage device
phi = 1

# Range query selectivity, 0 implies the key range per range query would roughly
# fit into 1 page. Selectivity 1 implies range queries will always query the
# full key possible range
s = 0.000000001

# Size of a single entry in bits
E = 8192

# Memory budget in terms of bits per element, this combined with number of
# elements (N) will get you roughly the expected total memory we are allocating
# between bloom filter memory and buffer memory
H = 10
# Can use ttotal memory instead of bits per element to budget, set M to -1 to
# use H as it will take priority
# M = 8589934592
M = -1

# Total number of elements our LSM tree will start off with
N = 1000000000
# N = 8589934592

# =============================================================================
# HEADER LCM
#   Add configurations related to learned cost models
# =============================================================================
[lcm]

# -----------------------------------------------------------------------------
# HEADER LCM.MODELS
#   Configuration for specific models
# -----------------------------------------------------------------------------
[lcm.model.classic]
out_dims = 4  # should match cost function output (z0, z1, q, h)
hidden_layers = 32

# Number of continuous variables the NN will take, referring to bits per
# element, workload, etc
num_cont_vars = 5

# Size of our cateogrical variable embedding, our overall cost-model network
# input will be
#   num_cont_vars + (num_cate_vars * embedding_size)
# As each categorical variable will be embedded down to size `embedding_size`
embedding_size = 32

[lcm.model.flexible]
out_dims = 4  # should match cost function output (z0, z1, q, h)

# Number of continuous variables the NN will take, referring to bits per
# element, workload, etc
num_cont_vars = 5

# Number of categorical variables we will need to encode, referring to size
# ratios and number of files per level
num_cate_vars = 2

# Hidden layers between the continous inputs + embedding to output
hidden_layers = 16
# Size of our cateogrical variable embedding, our overall cost-model network
# input will be
#   num_cont_vars + (num_cate_vars * embedding_size)
# As each categorical variable will be embedded down to size `embedding_size`
embedding_size = 16

# -----------------------------------------------------------------------------
# HEADER LCM.DATA
#   misc settings on data such as bias, precision, etc
# -----------------------------------------------------------------------------
[lcm.data]
# h, z0, z1, q, w
mean_bias = [5.0, 0.5, 0.5, 0.5, 0.5]
std_bias =  [2.88, 0.3, 0.3, 0.3, 0.3]
precision = 3

# =============================================================================
# END LCM
# =============================================================================

# =============================================================================
# HEADER LTUNE
#   Learned tuner module
# =============================================================================
[ltune]

# -----------------------------------------------------------------------------
# HEADER LTUNE.MODEL
#   Model configurations
# -----------------------------------------------------------------------------
[ltune.model]
in_dim = 4

[ltune.model.classic]
hidden_layers = 32

[ltune.model.klsm]
hidden_layers = 16

# -----------------------------------------------------------------------------
# HEADER LTUNE.DATA
#   Misc data items
# -----------------------------------------------------------------------------
[ltune.data]
# z0, z1, q, w
mean_bias = [0.5, 0.5, 0.5, 0.5]
std_bias = [0.3, 0.3, 0.3, 0.3]
precision = 3

# =============================================================================
# END LTUNE
# =============================================================================

# =============================================================================
# HEADER TRAIN
#   Generic settings for training options like optiimzers and schedulers
# =============================================================================
[train.scheduler.CosineAnnealingLR]
T_max = 5
eta_min = 0  # minimum learning rate

[train.optimizer.Adam]
lr = 0.01

[train.optimizer.SGD]
lr = 0.01

[train.optimizer.Adagrad]
lr = 0.01

# =============================================================================
# HEADER CREATETUNING
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[tunings]
# Architecture to pick
#   QCost - all levels same hold the same max files per level
#   KCost - multiple different max files per level
#   YZCost - Y files per level except the last level will be Z files
#   TierCost - Traditional model fixed to tiering policy
#   LevelCost - Traditional model fixed to leveling policy
#   TierLevelCost - Model will solve both and pick the lowest of the two
cost_model = "YZCost"

[tunings.rho]
# Range of rhos we want to search over over
start = 0
stop = 4
step = 0.25

# =============================================================================
# HEADER WORKLOADS
#     List of representative workloads
# =============================================================================
[[workloads]]
id = 0
z0 = 0.25
z1 = 0.25
q  = 0.25
w  = 0.25

[[workloads]]
id = 1
z0 = 0.97
z1 = 0.01
q = 0.01
w = 0.01

[[workloads]]
id = 2
z0 = 0.01
z1 = 0.97
q = 0.01
w = 0.01

[[workloads]]
id = 3
z0 = 0.01
z1 = 0.01
q = 0.97
w = 0.01

[[workloads]]
id = 4
z0 = 0.01
z1 = 0.01
q = 0.01
w = 0.97

[[workloads]]
id = 5
z0 = 0.49
z1 = 0.49
q = 0.01
w = 0.01

[[workloads]]
id = 6
z0 = 0.49
z1 = 0.01
q = 0.49
w = 0.01

[[workloads]]
id = 7
z0 = 0.49
z1 = 0.01
q = 0.01
w = 0.49

[[workloads]]
id = 8
z0 = 0.01
z1 = 0.49
q = 0.49
w = 0.01

[[workloads]]
id = 9
z0 = 0.01
z1 = 0.49
q = 0.01
w = 0.49

[[workloads]]
id = 10
z0 = 0.01
z1 = 0.01
q = 0.49
w = 0.49

[[workloads]]
id = 11
z0 = 0.33
z1 = 0.33
q = 0.33
w = 0.01

[[workloads]]
id = 12
z0 = 0.33
z1 = 0.33
q = 0.01
w = 0.33

[[workloads]]
id = 13
z0 = 0.33
z1 = 0.01
q = 0.33
w = 0.33

[[workloads]]
id = 14
z0 = 0.01
z1 = 0.33
q = 0.33
w = 0.33
