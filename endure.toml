# =============================================================================
# ENDURE Configuration File
#   Every job will contain it's own HEADER <Job Title> with settings appropiate
#   for the job.
#
# HEADER APP
#   Logic of app including jobs list to run
# =============================================================================
[app]
name = "ENDURE"

# =============================================================================
# HEADER LOGGER
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[log]
name = 'endure-logger'
format = "[%(levelname)s][%(asctime)-15s][%(filename)s] %(message)s"
datefmt = '%d-%m-%y:%H:%M:%S'
level = "DEBUG"
disable_tqdm = false

# =============================================================================
# HEADER IO
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[io]
data_dir = "data"

# =============================================================================
# HEADER JOB
#   Settings for each individual job (executable)
# =============================================================================
[job]
to_run = [
    # "LCMDataGen",
    # "LCMTrain",
    # "LTuneDataGen",
    # "LTuneTrain",
    "BayesianBaseline"
    # TODO: "CreateTunings",
    # TODO: "RunExperiments",
]

# -----------------------------------------------------------------------------
[job.LCMDataGen]
# -----------------------------------------------------------------------------
format = "parquet"
generator = "KHybridCost"
dir = "test-data/kcost-t30"
file_prefix = "kcost"
num_workers = -1 # -1 forces all cores to be used
num_files = 2
samples = 1024  # per file sample
overwrite_if_exists = true

# -----------------------------------------------------------------------------
[job.LCMTrain]
# -----------------------------------------------------------------------------
max_epochs = 5
save_dir = "models/lcm/kcost"
use_gpu_if_avail = false

# Model selection, picking "Auto" will give automatically select the model
# associated with the LSM design in the configuration file
model = "Auto"

# Different loss functions to train via
#   MSE - Mean squared error
#   NMSE - Normalized mean squared error
#   MSLE - Mean squared log error
#   RMSE - Root mean square error
#   RMSLE - Root mean squared log error
#   Huber - Huber loss
loss_fn = "MSE"

# Supported optimizers
#   SGD - Stochastic gradient descent
#   Adam
#   Adagrad
optimizer = "Adam"

# Learning rate schedulers
#   [CosineAnnealing, Exponential, Constant, None]
lr_scheduler = "Constant"

# Stop checkpointing to improve training
no_checkpoint = false

[job.LCMTrain.train]
dir = "train-data/kcost-t30"
format = "parquet"
batch_size = 32
shuffle = true
num_workers = 2
drop_last = true

[job.LCMTrain.test]
dir = "test-data/kcost-t30"
format = "parquet"
batch_size = 1024
shuffle = false
num_workers = 4
drop_last = true

# -----------------------------------------------------------------------------
[job.LTuneDataGen]
# -----------------------------------------------------------------------------
format = "parquet"
dir = "test-data/ltune/std"
file_prefix = "wl"
num_workers = 4  # -1 forces all cores to be used
num_files = 2
samples = 1024  # per file sample
overwrite_if_exists = true

# -----------------------------------------------------------------------------
[job.LTuneTrain]
# -----------------------------------------------------------------------------
max_epochs = 5
save_dir = "models/ltune/klsm"
use_gpu_if_avail = true

# Learned cost model is our loss, input full path to checkpoint or model file
loss_fn_path = "models/lcm/kcost"

# Check train.optimizer for available options
optimizer = "Adam"

# Learning rate schedulers
#   [CosineAnnealing, Exponential, Constant, None]
lr_scheduler = "Constant"

no_checkpoint = false

[job.LTuneTrain.train]
dir = "train-data/ltune/std"
format = "parquet"
batch_size = 2
shuffle = true
num_workers = 1
drop_last = true

[job.LTuneTrain.test]
dir = "test-data/ltune/std"
format = "parquet"
batch_size = 2
shuffle = false
num_workers = 1
drop_last = true

# -----------------------------------------------------------------------------
[job.BayesianOptimization]
# -----------------------------------------------------------------------------
num_iterations = 15
num_restarts = 20
# value of raw_samples determines how many initial random samples are taken from the search space before starting the optimization process
raw_samples = 30
initial_samples = 20

# This is the q value used in BoTorch Acquisition functions.
# if it is set to a value above 1 sequential processing will stop in acquisition function and batch processing will start
# note that for batch processing tensor shape will change and will require modification of code.
# TODO: Add code to handle batch
batch_size = 1
max_levels = 16

# Acquisition function options
# [ExpectedImprovement, UpperConfidenceBound, qExpectedImprovement]
acquisition_function = "ExpectedImprovement"
beta_value = 0.3
# model_type can take values - "Classic", "QHybrid", "YZHybrid", "KHybrid"
model_type = "YZHybrid"

[job.BayesianOptimization.database]
# This will take value 0 and 1 where 1 means write each cost and run details into the MySqlLite database
# and 0 means run details are not stored in the database
write_to_db = 1
# by default the databases directory will be created inside the data director. To change this, you need to change ["io"]["data_dir"]
db_path = "databases"
# This must be a .db file for code to function. It will create a sqllite database
db_name = "db_cost.db"

[job.BayesianOptimization.system]
E = 1024
s = 1.905581e-8
B = 64.0
N = 522365629
H = 5.705814
phi = 1.0

[job.BayesianOptimization.workload]
z0 = 0.063
z1 = 0.190
q = 0.545
w = 0.202

[job.BayesianOptimization.bounds]
h_min = 1.0
h_max = 10.0
T_min = 2.0
T_max = 31.0

# =============================================================================
# HEADER LSM
#   Generic LSM settings including maximum bounds, system settings, starting
#   budget for memory, number of elements, etc
# =============================================================================
[lsm]
# Design will effect everything else down stream (e.g. choice of neural network
# architecture for learned cost model)
#   Tier
#   Level
#   Classic
#   KHybrid
#   QFixed
#   YZHybrid
design = 'KHybrid'
max_levels = 20

[lsm.size_ratio]
max = 31
min = 2

[lsm.bits_per_elem]
max = 10
min = 0

[lsm.system]
# Number of physical entries per page
B = 4

# Read/Write asymmetry coefficient, 1 implies writes and reads throughput is
# roughly the same the current storage device
phi = 1

# Range query selectivity, 0 implies the key range per range Exponentialquery would roughly
# fit into 1 page. Selectivity 1 implies range queries will always query the
# full key possible range
s = 2e-7

# Size of a single entry in bits
E = 8192

# Memory budget in terms of bits per element, this combined with number of
# elements (N) will get you roughly the expected total memory we are allocating
# between bloom filter memory and buffer memory
H = 10
# Can use ttotal memory instead of bits per element to budget, set M to -1 to
# use H as it will take priority
# M = 8589934592
M = -1

# Total number of elements our LSM tree will start off with
N = 1000000000
# N = 8589934592

# =============================================================================
# HEADER LCM
#   Add configurations related to learned cost models
# =============================================================================
[lcm]
desc = 'Learned Cost Model'
# Input features for different dates is as follows:
#   Classic: ["z0", "z1", "q", "w", "B", "s", "E", "H", "N", "h", "policy", "T"]
#   QLSM: ["z0", "z1", "q", "w", "B", "s", "E", "H", "N", "h", "T", "Q"]
#   KLSM: ["z0", "z1", "q", "w", "B", "s", "E", "H", "N", "h", "T", "K"]
input_features = ["z0", "z1", "q", "w", "B", "s", "E", "H", "N", "h", "T", "K"]
output_features = ["z0_cost", "z1_cost", "q_cost", "w_cost"]

# -----------------------------------------------------------------------------
# HEADER LCM.MODELS
#   Configuration for specific models
# -----------------------------------------------------------------------------
[lcm.model]
embedding_size = 8
hidden_length = 3
hidden_width = 32
decision_dim = 64

# Dropout percentage
dropout = 0.0

# Batch or Layer norm
norm_layer = "Batch"

# Used only for classic models, generally smaller than embedding size
policy_embedding_size = 4

# -----------------------------------------------------------------------------
# HEADER LCM.DATA
#   misc settings on data such as bias, precision, etc
# -----------------------------------------------------------------------------
[lcm.data]
precision = 3
normalize_inputs = false

# =============================================================================
# END LCM
# =============================================================================

# =============================================================================
# HEADER LTUNE
#   Learned tuner module
# =============================================================================
[ltune]
desc = 'Learned Tuner'
penalty_factor = 10
input_features = ["z0", "z1", "q", "w", "B", "s", "E", "H", "N"]
output_features = ["h", "T", "Q"]
k_penalty = true

# -----------------------------------------------------------------------------
# HEADER LTUNE.MODEL
#   Model configurations
# -----------------------------------------------------------------------------
[ltune.model]
hidden_length = 1
hidden_width = 64

# Dropout percentage
dropout = 0

# Batch or Layer norm
norm_layer = "Batch"

categorical_mode = "reinmax"

# kwargs specific to LTune models during forward pass
[ltune.model.train_kwargs]
temp = 1
hard = false

[ltune.model.test_kwargs]
temp = 0.01
hard = true

# -----------------------------------------------------------------------------
# HEADER LTUNE.DATA
#   Misc data items
# -----------------------------------------------------------------------------
[ltune.data]
precision = 3
normalize_inputs = false

# =============================================================================
# END LTUNE
# =============================================================================

# =============================================================================
# HEADER TRAIN
#   Generic settings for training options like optiimzers and schedulers
# =============================================================================
[train.scheduler.CosineAnnealingLR]
T_max = 10
eta_min = 0.0001  # minimum learning rate

[train.scheduler.Exponential]
gamma = 0.9

[train.optimizer.Adam]
lr = 0.001

[train.optimizer.SGD]
lr = 0.001

[train.optimizer.Adagrad]
lr = 0.001


# =============================================================================
# HEADER LOSS
#   Settings for individual loss functions
# =============================================================================
[loss.Huber]
reduction = 'sum'
delta = 10

[loss.MSE]
reduction = 'mean'

# =============================================================================
# HEADER CREATETUNING
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[tunings]
# Architecture to pick
#   QCost - all levels same hold the same max files per level
#   KCost - multiple different max files per level
#   YZCost - Y files per level except the last level will be Z files
#   TierCost - Traditional model fixed to tiering policy
#   LevelCost - Traditional model fixed to leveling policy
#   TierLevelCost - Model will solve both and pick the lowest of the two
cost_model = "LevelCost"

[tunings.rho]
# Range of rhos we want to search over over
start = 0
stop = 4
step = 0.25

# =============================================================================
# HEADER GENERATOR
#     Generator settings
# =============================================================================
[generator]
page_sizes = [4, 8, 16]                     # KB pages
# selectivity_range = [1e-9, 1e-10]           # low, high
selectivity_range = [1e-7, 1e-9]            # low, high
entry_sizes = [1024, 2048, 4096, 8192]      # bits
memory_budget = [5, 20]                     # low, high, bits per element
elements_range = [100000000, 1000000000]    # element range

# =============================================================================
# HEADER WORKLOADS
#     List of representative workloads
# =============================================================================
[[workloads]]
id = 0
z0 = 0.25
z1 = 0.25
q  = 0.25
w  = 0.25

[[workloads]]
id = 1
z0 = 0.97
z1 = 0.01
q = 0.01
w = 0.01

[[workloads]]
id = 2
z0 = 0.01
z1 = 0.97
q = 0.01
w = 0.01

[[workloads]]
id = 3
z0 = 0.01
z1 = 0.01
q = 0.97
w = 0.01

[[workloads]]
id = 4
z0 = 0.01
z1 = 0.01
q = 0.01
w = 0.97

[[workloads]]
id = 5
z0 = 0.49
z1 = 0.49
q = 0.01
w = 0.01

[[workloads]]
id = 6
z0 = 0.49
z1 = 0.01
q = 0.49
w = 0.01

[[workloads]]
id = 7
z0 = 0.49
z1 = 0.01
q = 0.01
w = 0.49

[[workloads]]
id = 8
z0 = 0.01
z1 = 0.49
q = 0.49
w = 0.01

[[workloads]]
id = 9
z0 = 0.01
z1 = 0.49
q = 0.01
w = 0.49

[[workloads]]
id = 10
z0 = 0.01
z1 = 0.01
q = 0.49
w = 0.49

[[workloads]]
id = 11
z0 = 0.33
z1 = 0.33
q = 0.33
w = 0.01

[[workloads]]
id = 12
z0 = 0.33
z1 = 0.33
q = 0.01
w = 0.33

[[workloads]]
id = 13
z0 = 0.33
z1 = 0.01
q = 0.33
w = 0.33

[[workloads]]
id = 14
z0 = 0.01
z1 = 0.33
q = 0.33
w = 0.33
