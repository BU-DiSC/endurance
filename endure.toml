# =============================================================================
# ENDURE Configuration File
#   Following subsections are available
#   APP
#   LOGGER - output setting
#   IO - base directory for IO
#   LSM - Log structured merge tree assumptions and settings
#   JOB - all job specific settings
#   LCM - Learned cost model specifics
#   LTune - Learned tuner specifics
#   SCHEDULERS - ML learning rate schduler kwargs
#   OPTIMIZERS - ML optimizer kwargs
#   LOSS - ML Loss function kwargs
# =============================================================================

# =============================================================================
# HEADER APP
#   Logic of app including jobs list to run
# =============================================================================
[app]
name = "ENDURE"
run = [
    # "DataGen",
    # "LCMTrain",
    # "LTuneTrain",
    # "BayesianPipelineBoTorch",
    # "BayesianPipelineMLOS"
]

# =============================================================================
# HEADER LOGGER
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[log]
name = 'endure-logger'
format = "[%(levelname)s][%(asctime)-15s][%(filename)s] %(message)s"
datefmt = '%d-%m-%y:%H:%M:%S'
level = "DEBUG"
disable_tqdm = false

# =============================================================================
# HEADER IO
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[io]
data_dir = "/data"

# =============================================================================
# HEADER LSM
#   Generic LSM settings including maximum bounds, system settings, starting
#   budget for memory, number of elements, etc
# =============================================================================
[lsm]
# Design will effect everything else down stream (e.g. choice of neural network
# architecture for learned cost model)
#   Tiering
#   Leveling
#   Classic - Considers both leveing and tiering
#   QFixed - Levels 1 -> L = Q
#   YZHybrid - Levels 1 -> (L-1) = Q, Level L = Z
#   KHybrid - Each level has own K_i decision
design = 'QFixed'

[lsm.bounds]
max_considered_levels = 20                  # Max number of levels to consider
size_ratio_range = [2, 31]                  # low, high of size ratios to consider
page_sizes = [4, 8, 16]                     # KB pages
entry_sizes = [1024, 2048, 4096, 8192]      # bits
memory_budget_range = [5, 20]               # low, high, bits per element
selectivity_range = [1e-7, 1e-9]            # low, high
elements_range = [100000000, 1000000000]    # element range

# Default system values if not generating random systems
[lsm.system]
E = 1024           # size of a single entry in bits
s = 1.905581e-8    # range query selectivity, 1 implies the full key range per query
B = 64.0           # number of physical entries per page
N = 522365629      # total number of key-val pairs for LSM tree
H = 5.705814       # total memory budget in bits per element
phi = 1.0          # read/write asymmetry coefficient, 1 implies w/r cost the same

# Default workload if not generating from random distribution
[lsm.workload]
z0 = 0.063
z1 = 0.190
q = 0.545
w = 0.202

# =============================================================================
# HEADER JOB
#   Settings for each individual job (executable)
# =============================================================================
[job]
use_gpu_if_avail = false

# -----------------------------------------------------------------------------
[job.DataGen]
# -----------------------------------------------------------------------------
dir = "lcm/train/std"
generator = "LTuner"            # Select between data for tuner (LTuner) or LCM
file_prefix = "tuner"           # all files named file_prefix_000X.parquet
num_workers = -1                # -1 forces all cores to be used
num_files = 8                   # number of files to generate
samples = 1048576               # per file sample
overwrite_if_exists = true      # if files exist overwrite with new data

# -----------------------------------------------------------------------------
[job.LCMTrain]
# -----------------------------------------------------------------------------
max_epochs = 50
save_dir = "models/lcm/kcost"
no_checkpoint = false

# Different loss functions to train via
#   MSE - Mean squared error
#   NMSE - Normalized mean squared error
#   MSLE - Mean squared log error
#   RMSE - Root mean square error
#   RMSLE - Root mean squared log error
#   Huber - Huber loss
loss_fn = "MSE"

# Supported optimizers
#   [SGD, Adam, Adagrad]
optimizer = "Adam"

# Learning rate schedulers
#   [CosineAnnealing, Exponential, Constant, None]
lr_scheduler = "Constant"

[job.LCMTrain.train]
dir = "train-data/kcost-t30"
batch_size = 1024
shuffle = true
num_workers = 2
drop_last = true

[job.LCMTrain.test]
dir = "test-data/kcost-t30"
batch_size = 4096
shuffle = false
num_workers = 4
drop_last = true

# -----------------------------------------------------------------------------
[job.LTuneTrain]
# -----------------------------------------------------------------------------
max_epochs = 50
save_dir = "models/ltune/klsm"

# Learned cost model is our loss, input full path to checkpoint or model file
loss_fn_path = "models/lcm/kcost"

# Optimizer settings in header.optimizer
#   [SGD, Adam, Adagrad]
optimizer = "Adam"

# Learning rate schedulers, settings in header.scheduler
#   [CosineAnnealing, Exponential, Constant, None]
lr_scheduler = "Constant"

no_checkpoint = false

[job.LTuneTrain.train]
dir = "train-data/ltune/std"
batch_size = 1024
shuffle = true
num_workers = 1
drop_last = true

[job.LTuneTrain.test]
dir = "test-data/ltune/std"
batch_size = 4096
shuffle = false
num_workers = 1
drop_last = true

# -----------------------------------------------------------------------------
[job.BayesianOptimization]
# -----------------------------------------------------------------------------
num_iterations = 15
num_restarts = 2 # TODO set it to 2
# value of raw_samples determines how many initial random samples are taken from the search space before starting the optimization process
raw_samples = 3 # TODO set it to 3
initial_samples = 20
# for a true KLSM calculation - set num_k_values to the same value as max_levels. This is only consequential for the KLSM model
# This works in the following way:
# suppose num_k_values = 4 and max_levels = 20
# Then every layer till the 4th layer will have custom k values but the (num_k_values + 1) layer to the (max_levels) layer will only
# have a k value equal to 1
num_k_values = 4

# This is the q value used in BoTorch Acquisition functions.
# if it is set to a value above 1 sequential processing will stop in acquisition function and batch processing will start
# note that for batch processing tensor shape will change and will require modification of code.
# TODO: Add code to handle batch
batch_size = 1
# Acquisition function options
# [ExpectedImprovement, UpperConfidenceBound, qExpectedImprovement]
acquisition_function = "ExpectedImprovement"
beta_value = 0.3
# determines how many workloads do we want to test using the bayesian pipeline
multi_jobs_number = 100
multi_job_file = "design_comparison.csv"

[job.BayesianOptimization.database]
data_dir = "databases"
# This will take value 0 and 1 where 1 means write each cost and run details into the MySqlLite database
# and 0 means run details are not stored in the database
write_to_db = 1
# by default the databases directory will be created inside the data director. To change this, you need to change ["io"]["data_dir"]
db_path = "yz_databases"
# This must be a .db file for code to function. It will create a sqllite database
db_name = "yz_db_cost.db"

[job.BayesianOptimization.mlos]
#values can be Flaml, Smac, Random
optimizer = "Smac"
num_runs = 1000
iteration = 50
# =============================================================================
# HEADER LCM
#   Add configurations related to learned cost models
# =============================================================================
# -----------------------------------------------------------------------------
# HEADER LCM.MODELS
#   Configuration for specific models
# -----------------------------------------------------------------------------
[lcm.model]
embedding_size = 8
hidden_length = 3
hidden_width = 32
decision_dim = 64
dropout = 0.0           # dropout percentage
norm_layer = "Batch"    # "Batch" or "Layer" norm

# Used only for classic models, generally smaller than embedding size
policy_embedding_size = 4

# =============================================================================
# END LCM
# =============================================================================

# =============================================================================
# HEADER LTUNE
#   Learned tuner module
# =============================================================================
[ltune]
penalty_factor = 10

# kwargs specific to LTune models during forward pass
[ltune.train_kwargs]
temp = 1
hard = false

[ltune.test_kwargs]
temp = 0.01
hard = true

# -----------------------------------------------------------------------------
# HEADER LTUNE.MODEL
#   Model configurations
# -----------------------------------------------------------------------------
[ltune.model]
hidden_length = 1
hidden_width = 64
dropout = 0                     # dropout percentage
norm_layer = "Batch"            # batch or layer norm
categorical_mode = "reinmax"    # reinmax or gumbel

# =============================================================================
# END LTUNE
# =============================================================================

# =============================================================================
# HEADER SCHEDULERS
#   Specific settings for any learning rate schedulers
# =============================================================================
[scheduler.CosineAnnealingLR]
T_max = 10
eta_min = 0.0001  # minimum learning rate

[scheduler.Exponential]
gamma = 0.9

# =============================================================================
# HEADER OPTIMIZERS
#   Settings for any optimizers
# =============================================================================
[optimizer.Adam]
lr = 0.001

[optimizer.SGD]
lr = 0.001

[optimizer.Adagrad]
lr = 0.001

# =============================================================================
# HEADER LOSS
#   Settings for individual loss functions
# =============================================================================
[loss.Huber]
reduction = 'sum'
delta = 10

[loss.MSE]
reduction = 'mean'
